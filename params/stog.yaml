# Parameters of the Seq2Seq Model

# Device-sensitive parameters
amr_data_dir: &amr_data_dir data/all_amr
glove: &glove /home/sheng/data/glove/glove.840B.300d.zip


# Model parameters
model:
  model_type: STOG
  use_char_cnn: True

  encoder_token_embedding:
    num_embeddings:
    vocab_namespace: 'encoder_token_ids'
    embedding_dim: 300
    padding_index: 0
    dropout: 0.33
    pretrained_file: *glove

  encoder_char_embedding:
    num_embeddings:
    vocab_namespace: 'encoder_token_characters'
    embedding_dim: &encoder_char_embedding_dim 100
    padding_index: 0
    dropout: 0
    pretrained_file:

  encoder_char_cnn:
    embedding_dim: *encoder_char_embedding_dim
    num_filters: 100
    ngram_filter_sizes: [3]

  decoder_token_embedding:
    num_embeddings:
    vocab_namespace: 'decoder_token_ids'
    embedding_dim: 300
    padding_index: 0
    dropout: 0.33
    pretrained_file: *glove

  decoder_coref_embedding:
    num_embeddings: 500
    embedding_dim: 50
    padding_index: 0
    dropout: 0.33

  decoder_char_embedding:
    num_embeddings:
    vocab_namespace: 'decoder_token_characters'
    embedding_dim: &decoder_char_embedding_dim 100
    padding_index: 0
    dropout: 0.33
    pretrained_file:

  decoder_char_cnn:
    embedding_dim: *decoder_char_embedding_dim
    num_filters: 100
    ngram_filter_sizes: [3]

  encoder:
    input_size: 400
    hidden_size: 512
    num_layers: 2
    use_highway: False
    dropout: 0.33

  decoder:
    input_size: 1474
    hidden_size: &deocder_hidden_size 1024
    num_layers: 2
    use_highway: False
    dropout: 0.33

  source_attention:
    add_linear: True

  pointer_attention:
    add_linear: True

  generator:
    input_size: *deocder_hidden_size
    # Specify vocab_size and pad_idx dynamically
    vocab_size:
    pad_idx:
    force_copy: True

  graph_decoder:
    decode_algorithm: 'greedy'
    input_size: *deocder_hidden_size
    edge_node_hidden_size: 256
    edge_label_hidden_size: 128
    dropout: 0.33


# Vocabulary
vocab:
  min_count:
      encoder_token_ids: 1
      decoder_token_ids: 1
  max_vocab_size:
      encoder_token_ids: 17000
      decoder_token_ids: 14500


# Data parameters
data:
  data_dir: *amr_data_dir
  train_data: train_amr.txt.features.preproc
  dev_data: dev_amr.txt.features.preproc
  test_data: test_amr.txt.features.preproc
  data_type: AMR
  batch_first: True
  train_batch_size: &train_batch_size 32
  test_batch_size: 32
  iter_type: BucketIterator
  pretrain_token_emb: *glove
  pretrain_char_emb:


# Training parameters
environment:
  recover: False
  seed: 1
  numpy_seed: 1
  torch_seed: 1
  serialization_dir: &serialization_dir stog-ckpt
  file_friendly_logging: False
  gpu: True
  cuda_device: 0

trainer:
  device: # No need to be specified, will be updated at runtime
  # Optimizer
  no_grad: []
  optimizer_type: adam
  learning_rate: 0.001
  max_grad_norm: 5.0
  batch_size: *train_batch_size
  shuffle: True
  epochs: 50
  dev_metric: ppl
  serialization_dir: *serialization_dir
  model_save_interval:

test:
  evaluate_on_test: True
